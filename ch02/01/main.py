"""
æ­¥éª¤ 6: ä¸»ç¨‹åº (Main)
åŠŸèƒ½: æ‰§è¡Œå®Œæ•´çš„åˆ†è¯å™¨æµç¨‹ï¼ˆæ­¥éª¤ 1-5ï¼‰
"""

import os
import sys


def print_separator(title: str = "") -> None:
    """æ‰“å°åˆ†éš”çº¿"""
    print("\n" + "=" * 70)
    if title:
        print(f"  {title}")
        print("=" * 70)


def main():
    """æ‰§è¡Œå®Œæ•´çš„åˆ†è¯å™¨æ„å»ºæµç¨‹"""

    print("\n" + "ğŸš€" * 35)
    print(" " * 15 + "LLM åˆ†è¯å™¨å®Œæ•´æµç¨‹")
    print(" " * 10 + "æ‰§è¡Œæ­¥éª¤ 1-6ï¼šä»æ–‡ä»¶ç”Ÿæˆåˆ°åˆ†è¯å™¨æµ‹è¯•")
    print("ğŸš€" * 35)

    # ========== æ­¥éª¤ 1: ç”Ÿæˆæ–‡ä»¶ ==========
    print_separator("æ­¥éª¤ 1: ç”Ÿæˆ/ä¸‹è½½æ–‡ä»¶")
    from generate_file import generate_file
    file_path = generate_file()
    print(f"âœ“ æ­¥éª¤ 1 å®Œæˆ: æ–‡ä»¶å·²å‡†å¤‡å¥½")

    # ========== æ­¥éª¤ 2: è¯»å–æ–‡ä»¶ ==========
    print_separator("æ­¥éª¤ 2: è¯»å–æ–‡ä»¶å†…å®¹")
    from read_file import read_file
    raw_text = read_file()
    print(f"âœ“ æ­¥éª¤ 2 å®Œæˆ: æ–‡æœ¬å·²è¯»å– ({len(raw_text)} å­—ç¬¦)")

    # ========== æ­¥éª¤ 3: åˆ†è¯ ==========
    print_separator("æ­¥éª¤ 3: åˆ†è¯å¤„ç†")
    from tokenization import tokenize
    tokens = tokenize(raw_text)
    print(f"âœ“ æ­¥éª¤ 3 å®Œæˆ: è·å¾— {len(tokens)} ä¸ª tokens")

    # ========== æ­¥éª¤ 4: åˆ›å»ºè¯æ±‡è¡¨ ==========
    print_separator("æ­¥éª¤ 4: åˆ›å»ºè¯æ±‡è¡¨")
    from create_vocab import create_vocab
    vocab = create_vocab(tokens)
    print(f"âœ“ æ­¥éª¤ 4 å®Œæˆ: è¯æ±‡è¡¨åŒ…å« {len(vocab)} ä¸ªå”¯ä¸€ tokens")

    # ========== æ­¥éª¤ 5: å®ç°åˆ†è¯å™¨ç±» ==========
    print_separator("æ­¥éª¤ 5: åˆå§‹åŒ–åˆ†è¯å™¨")
    from tokenizer_class import SimpleTokenizerV1
    tokenizer = SimpleTokenizerV1(vocab)
    print(f"âœ“ æ­¥éª¤ 5 å®Œæˆ: åˆ†è¯å™¨å·²åˆ›å»º")

    # ========== æ­¥éª¤ 6: æµ‹è¯•åˆ†è¯å™¨ ==========
    print_separator("æ­¥éª¤ 6: æµ‹è¯•åˆ†è¯å™¨")

    # æµ‹è¯• 1: åŸºæœ¬ç¼–ç è§£ç 
    print("\n[æµ‹è¯• 1] åŸºæœ¬ç¼–ç è§£ç ")
    print("-" * 70)

    test_text = """It's the last he painted, you know," Mrs. Gisburn said with pardonable pride."""
    print(f"åŸæ–‡: {test_text}")

    # ç¼–ç 
    ids = tokenizer.encode(test_text)
    print(f"\nç¼–ç ç»“æœ:")
    print(f"  Token æ•°é‡: {len(ids)}")
    print(f"  Token IDs: {ids[:10]}... (æ˜¾ç¤ºå‰ 10 ä¸ª)")

    # è§£ç 
    decoded_text = tokenizer.decode(ids)
    print(f"\nè§£ç ç»“æœ:")
    print(f"  æ–‡æœ¬: {decoded_text}")
    print(f"  ä¸€è‡´æ€§: {'âœ“ é€šè¿‡' if test_text == decoded_text else 'âœ— å¤±è´¥'}")

    # æµ‹è¯• 2: ä½¿ç”¨è®­ç»ƒæ–‡æœ¬ä¸­çš„å¥å­
    print("\n[æµ‹è¯• 2] è®­ç»ƒæ–‡æœ¬å¥å­å¤„ç†")
    print("-" * 70)

    # ä»åŸå§‹æ–‡æœ¬ä¸­æå–ä¸€ä¸ªå¥å­è¿›è¡Œæµ‹è¯•
    test_text2 = "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough."
    print(f"åŸæ–‡: {test_text2}")

    try:
        ids2 = tokenizer.encode(test_text2)
        decoded2 = tokenizer.decode(ids2)

        print(f"\nToken æ•°é‡: {len(ids2)}")
        print(f"è§£ç æ–‡æœ¬: {decoded2}")
        print(f"ä¸€è‡´æ€§: {'âœ“ é€šè¿‡' if test_text2 == decoded2 else 'âœ— å¤±è´¥'}")
    except KeyError as e:
        print(f"\nâœ— æµ‹è¯•å¤±è´¥: è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨è¯ {e}")
        print("  (V1 ç‰ˆåˆ†è¯å™¨ä¸æ”¯æŒæœªçŸ¥è¯)")

    # æµ‹è¯• 3: å±•ç¤ºè¯æ±‡è¡¨ç»Ÿè®¡
    print("\n[æµ‹è¯• 3] è¯æ±‡è¡¨ç»Ÿè®¡")
    print("-" * 70)

    # ç»Ÿè®¡ä¿¡æ¯
    total_tokens = len(tokens)
    unique_tokens = len(vocab)
    avg_token_length = sum(len(t) for t in tokens) / len(tokens)

    print(f"  æ€» token æ•°: {total_tokens}")
    print(f"  å”¯ä¸€ token æ•°: {unique_tokens}")
    print(f"  å¹³å‡ token é•¿åº¦: {avg_token_length:.2f} å­—ç¬¦")

    # å±•ç¤ºä¸€äº›ç‰¹æ®Šçš„ tokens
    print(f"\n  ç‰¹æ®Š tokens ç¤ºä¾‹:")
    special_tokens = ['"', '--', '(', ')', ',', '.', '!', '?']
    for token in special_tokens:
        if token in vocab:
            print(f"    {repr(token):>6} -> ID: {vocab[token]:4d}")

    # ========== å®Œæˆ ==========
    print_separator("âœ¨ æ‰€æœ‰æ­¥éª¤å®Œæˆï¼")
    print("\nåˆ†è¯å™¨å·²æˆåŠŸæ„å»ºå¹¶æµ‹è¯•ï¼")
    print("\nğŸ“Š æ€»ç»“:")
    print(f"  â€¢ æ–‡ä»¶: {os.path.basename(file_path)}")
    print(f"  â€¢ æ–‡æœ¬å¤§å°: {len(raw_text)} å­—ç¬¦")
    print(f"  â€¢ Token æ€»æ•°: {len(tokens)}")
    print(f"  â€¢ è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
    print(f"  â€¢ åˆ†è¯å™¨: SimpleTokenizerV1")
    print("\n" + "ğŸ‰" * 35 + "\n")


if __name__ == "__main__":
    try:
        main()
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\nè¯·ç¡®ä¿æ‰€æœ‰æ­¥éª¤æ–‡ä»¶éƒ½åœ¨å½“å‰ç›®å½•ä¸­ï¼Œå¹¶ä¸”æŒ‰é¡ºåºæ‰§è¡Œã€‚")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
